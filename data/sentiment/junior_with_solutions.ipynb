{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с текстами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### библиотека NLTK - питоновская библиотека для манипуляцй с текстами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### токенизация: разбитие текста на токены\n",
    "Первый этап работы с текстовой информацией - выделение слов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPuckt tokenizer\n",
      "['Мама', 'мыла', 'раму']\n",
      "['Ростов', '-', 'на', '-', 'Дону']\n",
      "\n",
      "TreebankWord tokenizer\n",
      "['Мама', 'мыла', 'раму']\n",
      "['Ростов-на-Дону']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "print(\"WordPuckt tokenizer\")\n",
    "print(tokenizer.tokenize('Мама мыла раму'))\n",
    "print(tokenizer.tokenize('Ростов-на-Дону'))\n",
    "\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "print(\"\\nTreebankWord tokenizer\")\n",
    "print(tokenizer.tokenize('Мама мыла раму'))\n",
    "print(tokenizer.tokenize('Ростов-на-Дону'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Стемминг - приведение слов к нормальному виду\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['карл', 'у', 'клар', 'укра', 'коралл', ',', 'клар', 'у', 'карл', 'урка', 'кларнет', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "stemmer = RussianStemmer()\n",
    "tokens = tokenizer.tokenize('Карл у Клары украл кораллы, Клара у Карла уркала кларнет.')\n",
    "tokens_stem = [stemmer.stem(token.lower()) for token in tokens]\n",
    "print(tokens_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Мешок слов\n",
    "Простейшая модель для работы с текстами - 'мешок слов'. Идея заключается в том, что смысл текста можно восстановить даже не зная порядок слов, только зная их количества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': 1, 'у': 2, 'клар': 2, 'укра': 1, 'карл': 2, '.': 1, 'коралл': 1, 'урка': 1, 'кларнет': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tokens_bow = Counter(tokens_stem)\n",
    "print(dict(tokens_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Корпусом в текстовом анализе называают набор документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['бел', 'заяц', 'бел', ',', 'да', 'цен', 'ем', 'пятнадца', 'копеек', '.']\n",
      "['в', 'кулак', 'все', 'пальц', 'равн', '.']\n",
      "['глаз', 'стыдн', ',', 'а', 'душ', 'рад', '.']\n",
      "['где', 'дешев', ',', 'там', 'и', 'дор', '.']\n",
      "['дойдут', 'и', 'до', 'глух', 'вест', '.']\n",
      "['и', 'бородавк', 'тел', 'прибавк', '.']\n",
      "['куп', 'лишн', '-', 'прода', 'нужн', '.']\n",
      "['одн', 'мах', 'сто', 'побивах', ',', 'а', 'проч', 'не', 'считах', '.']\n",
      "['смерт', 'найдет', 'причин', '.']\n",
      "['у', 'наш', 'гришк', 'нет', 'отрыжк', '.']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "corpora = [\n",
    "    \"Белый заяц бел, да цена ему пятнадцать копеек.\",\n",
    "    \"В кулаке все пальцы равны.\",\n",
    "    \"Глазам стыдно, а душа радуется.\",\n",
    "    \"Где дешево, там и дорого.\",\n",
    "    \"Дойдут и до глухого вести.\",\n",
    "    \"И бородавка телу прибавка.\",\n",
    "    \"Купить лишнее - продать нужное.\",\n",
    "    \"Одним махом сто побивахом, а прочих не считахом.\",\n",
    "    \"Смерть найдет причину.\",\n",
    "    \"У нашего Гришки нет отрыжки.\"\n",
    "]\n",
    "\n",
    "corpora_tokenzied = [tokenizer.tokenize(doc.lower()) for doc in corpora]\n",
    "corpora_stemmed = []\n",
    "\n",
    "for doc in corpora_tokenzied:\n",
    "    stemmed_doc = [stemmer.stem(token) for token in doc]\n",
    "    corpora_stemmed.append(stemmed_doc)\n",
    "\n",
    "for doc in corpora_stemmed:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### библиотека gensim содержит некоторое количество математических моделей для работы с текстами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе с текстами не очень удобно использазовать строковые id, удобнее все слова языка пронумеровать и заменить их числами. В gensim для этого есть специальный класс Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(corpora_stemmed)\n",
    "print(dictionary.token2id['в'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь соберем преобразуем наш корпус в мешки слов. Для этого воспользуемся стандартным функционалом gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(1, 1), (8, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)],\n",
       " [(1, 1), (8, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)],\n",
       " [(8, 1), (21, 1), (24, 1), (25, 1), (26, 1), (27, 1)],\n",
       " [(8, 1), (21, 1), (28, 1), (29, 1), (30, 1)],\n",
       " [(8, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1)],\n",
       " [(1, 1),\n",
       "  (8, 1),\n",
       "  (18, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1)],\n",
       " [(8, 1), (43, 1), (44, 1), (45, 1)],\n",
       " [(8, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_bow = [dictionary.doc2bow(doc) for doc in corpora_stemmed]\n",
    "corpora_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно уже воспринимать документы не как тексты, а как вектор в N-мерном пространстве, в котором i-я координата означает количество раз, которое i-е слово встретилось в документе. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### частотный анализ. Посчитаем сколько раз каждое из слов встретилось в нашем корпусе\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 10\n",
      ", 4\n",
      "и 3\n",
      "бел 2\n",
      "а 2\n"
     ]
    }
   ],
   "source": [
    "result = Counter()\n",
    "for doc in corpora_bow:\n",
    "    result += Counter(dict(doc))\n",
    "\n",
    "for token_id, cnt in result.most_common(5):\n",
    "    print(dictionary[token_id], cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### считаем примеры текстов с которыми будем работать - база русскоязычных твитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "twits = [json.loads(line) for line in open('twits.json')]\n",
    "corpora = [twit['text'] for twit in twits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача №1. Посчитать частоты токенов в нашей базе. найти 20 самых частых. Для анализа использовать первых 5000 твитов\n",
    "В процессе построить мешки слов и словарь при помощи gensim по первым 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpora' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a7a0602443d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcorpora_tokenzied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcorpora_tokenzied\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpora' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "corpora_tokenzied = []\n",
    "for doc in tqdm(corpora[:10000]):\n",
    "    corpora_tokenzied.append(tokenizer.tokenize(doc.lower()))\n",
    "    \n",
    "corpora_stemmed = []\n",
    "\n",
    "for doc in tqdm(corpora_tokenzied):\n",
    "    stemmed_doc = [stemmer.stem(token) for token in doc]\n",
    "    corpora_stemmed.append(stemmed_doc)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(corpora_stemmed)\n",
    "corpora_bow = [dictionary.doc2bow(doc) for doc in corpora_stemmed]\n",
    "\n",
    "result = Counter()\n",
    "for doc in corpora_bow:\n",
    "    result += Counter(dict(doc))\n",
    "\n",
    "for token_id, cnt in result.most_common(20):\n",
    "    print(dictionary[token_id], cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "(dictionary, corpora_bow) = pickle.load(open('twits_bow.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# закон ципфа\n",
    "Закон Ципфа («ранг—частота») — эмпирическая закономерность распределения частоты слов естественного языка: если все слова языка (или просто достаточно длинного текста) упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n (так называемому рангу этого слова, см. шкала порядка)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "token_freq = Counter()\n",
    "for doc in tqdm(corpora_bow):\n",
    "    for token,cnt in doc:\n",
    "        token_freq[token] += cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "token_freq_list = token_freq.most_common(100)\n",
    "rank = list(range(len(token_freq_list)))\n",
    "freq = []\n",
    "for i in rank:\n",
    "    freq.append(1/token_freq_list[i][1])\n",
    "    \n",
    "plt.plot(rank, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самые частые слова являются наименее информативными. Хочется придумать какую-то модель, которая будет давать больший вес более редким словам и меньший вес более частым. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модели tf-idf вместо просто количества слова используем количество домноженное на коэффицент idf, который уменьшает вес частых слов\n",
    "\n",
    "![tf](https://wikimedia.org/api/rest_v1/media/math/render/svg/92a19022b85d3796b7e6237ea6829cb550ef17ff)\n",
    "![idf](https://wikimedia.org/api/rest_v1/media/math/render/svg/1c1f3347300bd19654bedfaef73861cf75ac5e65)\n",
    "\n",
    "doc[word] = tf[doc][word] * idf[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в gensim есть специальный модуль для работы с tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpora_bow)\n",
    "corpora_tfidf = tfidf[corpora_bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть хорошее представление наших твитов. Попробуем обучить логистическую регрессию, которая научится отличать негативные твиты от позитивных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-59f50ecfb767>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtwit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtwit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtwits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'twits' is not defined"
     ]
    }
   ],
   "source": [
    "target = [twit['sentiment'] for twit in twits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "try:\n",
    "    (data, col, row) = pickle.load(open('matrix.pickle', 'rb'))\n",
    "except:\n",
    "    data = []\n",
    "    col = []\n",
    "    row = []\n",
    "    for i in tqdm(range(len(corpora_tfidf))):\n",
    "        for j in range(len(corpora_tfidf[i])):\n",
    "            data.append(corpora_tfidf[i][j][1])\n",
    "            col.append(corpora_tfidf[i][j][0])\n",
    "            row.append(i)\n",
    "    pickle.dump((data, col, row), open('matrix.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix = csc_matrix((data, (row, col)), shape=(len(corpora_tfidf), len(dictionary)))\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(matrix, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8fa82fc5a78f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcoef_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcoef_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoef_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "coef_weight = list(enumerate(classifier.coef_[0]))\n",
    "coef_weight.sort(key=lambda x: x[1])\n",
    "for word in coef_weight[:50]:\n",
    "    print(dictionary[word[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    tokens = [stemmer.stem(token) for token in tokenizer.tokenize(text.lower())]\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    tfidf_vec = tfidf[bow]\n",
    "    data = []\n",
    "    col = []\n",
    "    row = []\n",
    "    for token in tfidf_vec:\n",
    "        data.append(token[1])\n",
    "        col.append(token[0])\n",
    "        row.append(0)\n",
    "    matrix_1 = csc_matrix((data, (row, col)), shape=(1, len(dictionary)))\n",
    "    return(classifier.predict_proba(matrix_1)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.628812956025\n"
     ]
    }
   ],
   "source": [
    "print(classify(\"наконец-то 0.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1551585591\n"
     ]
    }
   ],
   "source": [
    "print(classify(\"хочу умереть, ненавижу всех\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Разбить выборку на train и test и померить качество\n",
    "Воспользоваться методами sklearn.cross_validation.train_test_split для разбивки и sclearn.metrics.roc_auc_score для измерения качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99990025221766865"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(matrix, target)\n",
    "clsf = LogisticRegression()\n",
    "clsf.fit(data_train, target_train)\n",
    "result = clsf.predict_proba(data_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(target_test, result[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем выяснить какие темы волнуют пользователей твиттера. Для этого воспользуемся моделью latent dirichlet allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    lda = pickle.load(open(\"lda_model.pickle\", 'rb'))\n",
    "except:\n",
    "    lda = gensim.models.ldamulticore.LdaMulticore(corpus=corpora_tfidf, id2word=dictionary)\n",
    "    pickle.dump(lda, open(\"lda_model.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, '0.009*( + 0.007*) + 0.007*зуб + 0.006*, + 0.006*замерзл + 0.005*! + 0.005*в + 0.004*и + 0.004*я + 0.004*не')\n",
      "(62, '0.010*английск + 0.010*( + 0.007*черт + 0.006*заболет + 0.006*) + 0.006*, + 0.005*одиночеств + 0.005*болел + 0.005*: + 0.004*не')\n",
      "(98, '0.010*учеб + 0.010*( + 0.009*ретв + 0.007*) + 0.006*проеба + 0.005*заснут + 0.005*, + 0.005*останов + 0.005*похож + 0.005*@')\n",
      "(60, '0.014*тяжел + 0.008*( + 0.006*глуп + 0.006*дни + 0.006*) + 0.005*, + 0.005*! + 0.005*я + 0.005*не + 0.005*в')\n",
      "(92, '0.014*( + 0.009*зачет + 0.007*! + 0.006*, + 0.006*9 + 0.006*встрет + 0.006*не + 0.006*) + 0.006*числ + 0.005*я')\n",
      "(2, '0.013*( + 0.010*чет + 0.010*шерлок + 0.008*отвеча + 0.007*экзамен + 0.006*смогл + 0.006*уста + 0.006*врач + 0.005*не + 0.005*учител')\n",
      "(36, '0.008*пизд + 0.007*где-т + 0.007*) + 0.006*приложен + 0.005*пролетел + 0.005*( + 0.005*: + 0.005*, + 0.005*@ + 0.005*лер')\n",
      "(15, '0.011*фу + 0.008*( + 0.007*увидет + 0.007*аааа + 0.007*! + 0.006*трен + 0.006*) + 0.005*вспомина + 0.005*, + 0.005*не')\n",
      "(10, '0.017*больн + 0.011*( + 0.007*! + 0.006*, + 0.006*) + 0.006*я + 0.005*и + 0.005*не + 0.005*: + 0.005*?')\n",
      "(14, '0.009*( + 0.008*волгоград + 0.008*a + 0.007*q + 0.007*) + 0.006*наушник + 0.006*: + 0.006*зада + 0.006*кроват + 0.005*,')\n",
      "(70, '0.025*| + 0.012*идт + 0.010*( + 0.009*елк + 0.008*никуд + 0.007*не + 0.006*, + 0.005*: + 0.005*в + 0.005*!')\n",
      "(48, '0.012*слез + 0.009*( + 0.009*комп + 0.007*выздоравлива + 0.006*) + 0.006*фак + 0.006*! + 0.005*, + 0.005*@ + 0.005*зло')\n",
      "(50, '0.010*биолог + 0.009*везет + 0.009*везд + 0.006*( + 0.006*) + 0.005*! + 0.005*, + 0.005*прочита + 0.004*обновля + 0.004*пришел')\n",
      "(40, '0.030*ужасн + 0.008*( + 0.008*тви + 0.007*стих + 0.007*руга + 0.006*) + 0.005*! + 0.005*: + 0.005*переда + 0.005*@')\n",
      "(1, '0.008*живот + 0.008*измен + 0.007*( + 0.007*страда + 0.006*) + 0.005*показыва + 0.005*, + 0.005*40 + 0.004*: + 0.004*не')\n",
      "(85, '0.015*плака + 0.009*( + 0.008*пздц + 0.008*заход + 0.008*умер + 0.006*закр + 0.006*твор + 0.005*географ + 0.005*не + 0.005*!')\n",
      "(51, '0.009*( + 0.008*блиин + 0.006*) + 0.005*, + 0.005*! + 0.005*не + 0.005*увидел + 0.005*@ + 0.005*я + 0.005*:')\n",
      "(58, '0.008*( + 0.008*зря + 0.006*) + 0.006*, + 0.005*олимпиад + 0.005*не + 0.005*я + 0.005*опозда + 0.005*! + 0.005*шикарн')\n",
      "(79, '0.010*алгебр + 0.009*обидел + 0.009*заканчива + 0.009*( + 0.006*страшн + 0.006*единствен + 0.006*) + 0.005*! + 0.005*предмет + 0.005*,')\n",
      "(63, '0.023*жалк + 0.009*слома + 0.008*( + 0.008*# + 0.008*контрольн + 0.006*уб + 0.006*) + 0.005*показа + 0.005*: + 0.005*,')\n",
      "(35, '0.008*шутк + 0.007*( + 0.007*ленив + 0.006*трудн + 0.006*) + 0.005*пишеш + 0.005*пит + 0.005*31 + 0.005*уу + 0.005*:')\n",
      "(68, '0.009*( + 0.008*порт + 0.008*оценк + 0.007*проход + 0.007*далек + 0.006*) + 0.006*депресс + 0.006*прошел + 0.006*! + 0.005*отда')\n",
      "(67, '0.012*( + 0.008*одноклассник + 0.006*) + 0.006*13 + 0.006*! + 0.006*, + 0.005*999 + 0.005*пятниц + 0.005*не + 0.005*:')\n",
      "(12, '0.010*( + 0.009*господ + 0.006*, + 0.006*) + 0.006*! + 0.005*сочинен + 0.005*не + 0.005*засыпа + 0.005*я + 0.005*в')\n",
      "(0, '0.010*( + 0.009*хренов + 0.008*бл + 0.007*жест + 0.006*) + 0.006*! + 0.006*херн + 0.005*обновлен + 0.005*не + 0.005*,')\n",
      "(4, '0.010*( + 0.008*убира + 0.007*лож + 0.007*сочувств + 0.006*) + 0.006*печальк + 0.005*, + 0.005*перед + 0.005*я + 0.005*не')\n",
      "(43, '0.010*( + 0.008*брос + 0.007*ващ + 0.006*) + 0.005*нет. + 0.005*, + 0.005*подписчик + 0.005*пропуска + 0.005*@ + 0.004*не')\n",
      "(39, '0.007*) + 0.006*неожида + 0.006*идут + 0.006*( + 0.005*! + 0.005*@ + 0.005*, + 0.005*: + 0.004*prisonero_ + 0.004*украс')\n",
      "(78, '0.009*потеря + 0.008*( + 0.006*) + 0.006*! + 0.005*, + 0.005*важн + 0.005*не + 0.005*вперв + 0.005*зла + 0.005*ум')\n",
      "(6, '0.014*уеха + 0.013*уезжа + 0.009*( + 0.006*дур + 0.006*питер + 0.006*) + 0.006*! + 0.006*, + 0.005*срочн + 0.005*в')\n"
     ]
    }
   ],
   "source": [
    "for topic in lda.print_topics(30):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
